{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba395739",
   "metadata": {},
   "source": [
    "# Model Diagnostics & Validation\n",
    "\n",
    "This notebook demonstrates model quality assessment and inference diagnostics.\n",
    "\n",
    "**Topics**:\n",
    "1. Posterior trace diagnostics (Rhat, ESS, divergences)\n",
    "2. Posterior predictive checks\n",
    "3. Prediction calibration\n",
    "4. Model comparison (static vs time-varying, with/without defense)\n",
    "5. Train/test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rugby_ranking.notebook_utils import setup_notebook_environment, load_model_and_trace, print_summary\n",
    "from rugby_ranking.model.validation import (\n",
    "    temporal_train_test_split,\n",
    "    validate_predictions,\n",
    "    log_likelihood_scores,\n",
    ")\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup: load data and configure plots\n",
    "dataset, df, model_dir = setup_notebook_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9e781",
   "metadata": {},
   "source": [
    "## 1. Load Model\n",
    "\n",
    "Load a trained model checkpoint and examine its inference configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, trace = load_model_and_trace(\"latest\")\n",
    "\n",
    "print(f\"Model Type: {'Time-varying' if model.config.time_varying_effects else 'Static'}\")\n",
    "print(f\"Separate Kicking Effects: {model.config.separate_kicking_effect}\")\n",
    "print(f\"Include Defense: {model.config.include_defense}\")\n",
    "print(f\"\\nPosterior dimensions:\")\n",
    "print(f\"  Chains: {trace.posterior.dims['chain']}\")\n",
    "print(f\"  Draws: {trace.posterior.dims['draw']}\")\n",
    "print(f\"  Warmup: {trace.posterior.dims.get('warmup', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4e8cc",
   "metadata": {},
   "source": [
    "## 2. Trace Diagnostics\n",
    "\n",
    "Assess whether the MCMC chain has converged using Rhat (should be < 1.01) and ESS (effective sample size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key parameters\n",
    "summary = az.summary(\n",
    "    trace,\n",
    "    var_names=['alpha', 'sigma_player_try', 'sigma_team'],\n",
    "    kind='stats',\n",
    ")\n",
    "\n",
    "print(\"Key Parameter Summary:\")\n",
    "print(summary)\n",
    "\n",
    "# Check for convergence issues\n",
    "print(\"\\nConvergence Check:\")\n",
    "rhat = summary['r_hat']\n",
    "problems = (rhat > 1.01).sum()\n",
    "if problems == 0:\n",
    "    print(\"✓ All Rhat < 1.01 (good convergence)\")\n",
    "else:\n",
    "    print(f\"✗ {problems} parameters have Rhat > 1.01 (check convergence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESS Ratio (effective sample size / total samples)\n",
    "print(\"ESS Ratios (should be > 0.1):\")\n",
    "ess_bulk = summary['ess_bulk'] / (trace.posterior.dims['chain'] * trace.posterior.dims['draw'])\n",
    "ess_tail = summary['ess_tail'] / (trace.posterior.dims['chain'] * trace.posterior.dims['draw'])\n",
    "\n",
    "print(f\"  Bulk: {ess_bulk.min():.3f} - {ess_bulk.max():.3f}\")\n",
    "print(f\"  Tail: {ess_tail.min():.3f} - {ess_tail.max():.3f}\")\n",
    "\n",
    "low_ess = (ess_bulk < 0.1).sum() + (ess_tail < 0.1).sum()\n",
    "if low_ess == 0:\n",
    "    print(\"✓ All ESS ratios acceptable\")\n",
    "else:\n",
    "    print(f\"⚠️  {low_ess} parameters have low ESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a3f7f",
   "metadata": {},
   "source": [
    "## 3. Divergences\n",
    "\n",
    "Check for post-warmup divergences (indicates sampling difficulties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for divergences\n",
    "if 'diverging' in trace.sample_stats.data_vars:\n",
    "    divergences = trace.sample_stats.diverging.sum()\n",
    "    total = trace.posterior.dims['chain'] * trace.posterior.dims['draw']\n",
    "    div_pct = (divergences / total) * 100\n",
    "    \n",
    "    if divergences == 0:\n",
    "        print(f\"✓ No divergences (good)\")\n",
    "    elif div_pct < 1:\n",
    "        print(f\"⚠️  {divergences} divergences ({div_pct:.1f}%, acceptable)\")\n",
    "    else:\n",
    "        print(f\"✗ {divergences} divergences ({div_pct:.1f}%, consider re-tuning)\")\n",
    "else:\n",
    "    print(\"(Divergence information not available in trace)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126ce7a",
   "metadata": {},
   "source": [
    "## 4. Posterior Predictive Checks\n",
    "\n",
    "Compare observed data to predictions from the posterior to check model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement posterior predictive checks\n",
    "# This requires computing predictions for each posterior sample\n",
    "# and comparing to observed scoring events\n",
    "\n",
    "print(\"Posterior predictive checks not yet implemented.\")\n",
    "print(\"This would compare observed vs predicted score distributions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b64d5",
   "metadata": {},
   "source": [
    "## 5. Prediction Calibration\n",
    "\n",
    "Assess whether predicted probabilities match observed frequencies on held-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal train/test split\n",
    "all_dates = df['date'].unique()\n",
    "split_date = pd.Timestamp(np.percentile(all_dates, 80))\n",
    "\n",
    "train_df = df[df['date'] < split_date].copy()\n",
    "test_df = df[df['date'] >= split_date].copy()\n",
    "\n",
    "print(f\"Train set: {train_df['date'].min().date()} to {train_df['date'].max().date()} ({len(train_df)} records)\")\n",
    "print(f\"Test set:  {test_df['date'].min().date()} to {test_df['date'].max().date()} ({len(test_df)} records)\")\n",
    "print(f\"\\nTest set contains {test_df['player_name'].nunique()} players, {test_df.groupby(['date', 'team']).ngroups} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute predictions on test set\n",
    "# This would predict matches in the test set and compare to actual outcomes\n",
    "\n",
    "print(\"Calibration analysis not yet implemented.\")\n",
    "print(\"This would compute prediction accuracy and probability calibration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e329466",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare different model variants (static vs time-varying, with/without defense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load multiple models and compare\n",
    "# models = {\n",
    "#     'static': load_model_and_trace('static_model'),\n",
    "#     'time_varying': load_model_and_trace('timevarying_model'),\n",
    "#     'defense': load_model_and_trace('defense_model'),\n",
    "# }\n",
    "\n",
    "print(\"Model comparison not yet implemented.\")\n",
    "print(\"This would compare LOO-CV scores and other metrics across models.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
