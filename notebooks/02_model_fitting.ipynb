{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting and Output Exploration\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load data and fit the hierarchical Bayesian model\n",
    "2. Examine convergence diagnostics\n",
    "3. Extract and visualize player/team rankings\n",
    "4. Generate match predictions\n",
    "5. Perform posterior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "\n",
    "from rugby_ranking.model.data import MatchDataset\n",
    "from rugby_ranking.model.core import RugbyModel, ModelConfig\n",
    "from rugby_ranking.model.inference import ModelFitter, InferenceConfig\n",
    "from rugby_ranking.model.predictions import MatchPredictor\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "az.style.use('arviz-darkgrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../../Rugby-Data\")  # Adjust path as needed\n",
    "\n",
    "dataset = MatchDataset(DATA_DIR)\n",
    "dataset.load_json_files()\n",
    "\n",
    "df = dataset.to_dataframe(played_only=True)\n",
    "\n",
    "# Filter to valid positions (1-23)\n",
    "df = df[df['position'].between(1, 23)].copy()\n",
    "\n",
    "print(f\"Observations: {len(df):,}\")\n",
    "print(f\"Players: {df['player_name'].nunique():,}\")\n",
    "print(f\"Teams: {df['team'].nunique()}\")\n",
    "print(f\"Matches: {df['match_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Build and Fit Model\n\nWe use the joint model which shares player and team effects across all scoring types (tries, penalties, conversions, drop goals). This provides better estimates especially for kickers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure the model for joint scoring types\nconfig = ModelConfig(\n    score_types=(\"tries\", \"penalties\", \"conversions\", \"drop_goals\"),\n    player_effect_sd=0.5,\n    team_effect_sd=0.3,\n    position_effect_sd=0.5,\n)\n\nmodel = RugbyModel(config)\npymc_model = model.build_joint(df)\n\nprint(\"Joint model built successfully\")\nprint(f\"  Players: {len(model._player_ids)}\")\nprint(f\"  Team-seasons: {len(model._team_season_ids)}\")\nprint(f\"  Score types: {config.score_types}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fit using Variational Inference (fast)\ninference_config = InferenceConfig(\n    vi_n_iterations=50000,  # More iterations for joint model\n    vi_method=\"advi\",\n)\n\nfitter = ModelFitter(model, inference_config)\n\nprint(\"Fitting joint model with VI (this may take a few minutes)...\")\ntrace = fitter.fit_vi(n_samples=2000)\nprint(\"Done!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Full MCMC (slower but more accurate)\n",
    "\n",
    "Uncomment to run full MCMC sampling. This takes longer but provides better uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Full MCMC (uncomment to run)\n",
    "# inference_config = InferenceConfig(\n",
    "#     mcmc_draws=1000,\n",
    "#     mcmc_tune=500,\n",
    "#     mcmc_chains=4,\n",
    "#     mcmc_cores=4,\n",
    "# )\n",
    "# fitter = ModelFitter(model, inference_config)\n",
    "# trace = fitter.fit_mcmc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "diag = fitter.diagnostics()\n",
    "print(\"Convergence Diagnostics:\")\n",
    "print(f\"  R-hat max: {diag['r_hat_max']:.4f} (should be < 1.01)\")\n",
    "print(f\"  R-hat mean: {diag['r_hat_mean']:.4f}\")\n",
    "print(f\"  ESS bulk min: {diag['ess_bulk_min']:.0f} (should be > 400)\")\n",
    "print(f\"  ESS tail min: {diag['ess_tail_min']:.0f}\")\n",
    "print(f\"  Method: {diag['fit_method']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ArviZ summary for key parameters\n# For joint model, show parameters across score types\nsummary = az.summary(trace, var_names=['alpha', 'eta_home', 'sigma_player', 'sigma_team', 'lambda_player', 'lambda_team'])\ndisplay(summary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Posterior distributions for global parameters\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Alpha (baseline rates) for each score type\nalpha = trace.posterior['alpha'].values\nscore_types = config.score_types\nfor i, st in enumerate(score_types):\n    axes[0, 0].hist(alpha[:, :, i].flatten(), bins=30, alpha=0.5, label=st, density=True)\naxes[0, 0].set_title('Baseline Log-Rate (α) by Score Type')\naxes[0, 0].legend()\n\n# Home advantage for each score type\neta = trace.posterior['eta_home'].values\nfor i, st in enumerate(score_types):\n    axes[0, 1].hist(eta[:, :, i].flatten(), bins=30, alpha=0.5, label=st, density=True)\naxes[0, 1].set_title('Home Advantage (η) by Score Type')\naxes[0, 1].legend()\n\naz.plot_posterior(trace, var_names=['sigma_player'], ax=axes[1, 0])\naxes[1, 0].set_title('Player Effect SD (σ_player)')\n\naz.plot_posterior(trace, var_names=['sigma_team'], ax=axes[1, 1])\naxes[1, 1].set_title('Team Effect SD (σ_team)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Player Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Top try-scorers by estimated ability (from joint model)\nplayer_rankings = model.get_player_rankings(top_n=30, score_type=\"tries\")\nprint(\"Top Try Scorers:\")\ndisplay(player_rankings)\n\n# Top kickers (penalties + conversions)\n# For kickers, we can look at penalty scoring ability\npenalty_rankings = model.get_player_rankings(top_n=20, score_type=\"penalties\")\nprint(\"\\nTop Penalty Kickers:\")\ndisplay(penalty_rankings)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize top try-scorers with uncertainty\ntop_n = 20\nrankings = model.get_player_rankings(top_n=top_n, score_type=\"tries\")\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\ny_pos = np.arange(top_n)\nax.barh(y_pos, rankings['effect_mean'], xerr=rankings['effect_std'] * 1.96, \n        align='center', alpha=0.7, capsize=3)\nax.set_yticks(y_pos)\nax.set_yticklabels(rankings['player'])\nax.invert_yaxis()\nax.set_xlabel('Player Effect (log-rate)')\nax.set_title(f'Top {top_n} Try Scorers (with 95% CI)')\nax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Player effect distribution (shared latent ability)\nbeta_raw = trace.posterior['beta_player_raw'].values.flatten()\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.hist(beta_raw, bins=50, density=True, alpha=0.7, edgecolor='black')\nax.set_xlabel('Raw Player Effect (latent ability)')\nax.set_ylabel('Density')\nax.set_title('Distribution of Latent Player Ability')\nax.axvline(x=0, color='red', linestyle='--', label='Average')\nax.legend()\nplt.show()\n\nprint(f\"Raw player effect range: [{beta_raw.min():.2f}, {beta_raw.max():.2f}]\")\nprint(f\"Raw player effect std: {beta_raw.std():.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Team Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most recent season\n",
    "seasons = sorted(model._season_ids.keys())\n",
    "current_season = seasons[-1] if seasons else None\n",
    "print(f\"Current season: {current_season}\")\n",
    "\n",
    "# Team rankings for current season\n",
    "team_rankings = model.get_team_rankings(season=current_season, top_n=20)\n",
    "display(team_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize team rankings\n",
    "if len(team_rankings) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    y_pos = np.arange(len(team_rankings))\n",
    "    colors = ['green' if x > 0 else 'red' for x in team_rankings['effect_mean']]\n",
    "    \n",
    "    ax.barh(y_pos, team_rankings['effect_mean'], xerr=team_rankings['effect_std'] * 1.96,\n",
    "            align='center', alpha=0.7, capsize=3, color=colors)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(team_rankings['team'])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Team Effect (log-rate)')\n",
    "    ax.set_title(f'Team Rankings - {current_season} (with 95% CI)')\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Position Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract position effects for each score type\ntheta = trace.posterior['theta_position'].values  # (chain, draw, n_score_types, n_positions)\n\npositions = {\n    1: 'LH Prop', 2: 'Hooker', 3: 'TH Prop',\n    4: 'Lock', 5: 'Lock', 6: 'Blindside', 7: 'Openside', 8: 'No.8',\n    9: 'Scrum-half', 10: 'Fly-half', 11: 'Left Wing',\n    12: 'Inside Ctr', 13: 'Outside Ctr', 14: 'Right Wing', 15: 'Fullback',\n    16: 'Rep HK', 17: 'Rep LH', 18: 'Rep TH', 19: 'Rep LK', 20: 'Rep BR',\n    21: 'Rep SH', 22: 'Rep FH', 23: 'Rep BK'\n}\n\n# Plot position effects for tries vs kicks\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\nfor s, (st, ax) in enumerate(zip(config.score_types, axes.flat)):\n    theta_mean = theta[:, :, s, :].mean(axis=(0, 1))\n    theta_std = theta[:, :, s, :].std(axis=(0, 1))\n    \n    x_pos = np.arange(23)\n    colors = ['#1f77b4' if i < 8 else '#2ca02c' if i < 15 else '#7f7f7f' for i in range(23)]\n    \n    ax.bar(x_pos, theta_mean, yerr=theta_std * 1.96, capsize=2, alpha=0.7, color=colors)\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels([positions.get(i+1, str(i+1)) for i in range(23)], rotation=45, ha='right', fontsize=8)\n    ax.set_ylabel('Position Effect (log-rate)')\n    ax.set_title(f'{st.capitalize()} Rate by Position (with 95% CI)')\n    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Match Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor\n",
    "predictor = MatchPredictor(model, trace)\n",
    "\n",
    "# Example prediction (teams only)\n",
    "# Find some teams that exist in the current season\n",
    "current_teams = [ts[0] for ts in model._team_season_ids.keys() if ts[1] == current_season]\n",
    "print(f\"Teams in {current_season}: {current_teams[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a match (adjust team names as needed)\n",
    "if len(current_teams) >= 2:\n",
    "    home_team = current_teams[0]\n",
    "    away_team = current_teams[1]\n",
    "    \n",
    "    try:\n",
    "        prediction = predictor.predict_teams_only(\n",
    "            home_team=home_team,\n",
    "            away_team=away_team,\n",
    "            season=current_season,\n",
    "            n_samples=2000\n",
    "        )\n",
    "        \n",
    "        print(prediction.summary())\n",
    "        print()\n",
    "        print(f\"Detailed breakdown:\")\n",
    "        print(f\"  Home ({home_team}): {prediction.home.mean:.1f} ± {prediction.home.std:.1f}\")\n",
    "        print(f\"  Away ({away_team}): {prediction.away.mean:.1f} ± {prediction.away.std:.1f}\")\n",
    "        print(f\"  90% CI Home: [{prediction.home.ci_lower:.0f}, {prediction.home.ci_upper:.0f}]\")\n",
    "        print(f\"  90% CI Away: [{prediction.away.ci_lower:.0f}, {prediction.away.ci_upper:.0f}]\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not predict: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "if 'prediction' in dir() and prediction is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Score distributions\n",
    "    axes[0].hist(prediction.home.samples, bins=30, alpha=0.6, label=home_team, density=True)\n",
    "    axes[0].hist(prediction.away.samples, bins=30, alpha=0.6, label=away_team, density=True)\n",
    "    axes[0].set_xlabel('Predicted Score')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title('Score Distributions')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Margin distribution\n",
    "    margin = prediction.home.samples - prediction.away.samples\n",
    "    axes[1].hist(margin, bins=40, alpha=0.7, color='purple', density=True)\n",
    "    axes[1].axvline(x=0, color='red', linestyle='--', label='Draw')\n",
    "    axes[1].axvline(x=margin.mean(), color='green', linestyle='-', label=f'Mean: {margin.mean():.1f}')\n",
    "    axes[1].set_xlabel('Margin (Home - Away)')\n",
    "    axes[1].set_ylabel('Density')\n",
    "    axes[1].set_title('Predicted Margin')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Posterior Predictive Checks\n",
    "\n",
    "Compare model predictions to observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare observed vs predicted rates by position for each score type\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\nfor s, (st, ax) in enumerate(zip(config.score_types, axes.flat)):\n    # Observed rates\n    observed_rates = df.groupby('position').agg({\n        st: 'sum',\n        'minutes_played': 'sum'\n    })\n    observed_rates['rate_per_80'] = observed_rates[st] / observed_rates['minutes_played'] * 80\n    \n    # Get model predictions for each position\n    alpha_mean = trace.posterior['alpha'].values[:, :, s].mean()\n    theta_mean = trace.posterior['theta_position'].values[:, :, s, :].mean(axis=(0, 1))\n    \n    predicted_rate = np.exp(alpha_mean + theta_mean)\n    \n    positions_list = list(range(1, 24))\n    x = np.arange(len(positions_list))\n    width = 0.35\n    \n    obs_rates = [observed_rates.loc[p, 'rate_per_80'] if p in observed_rates.index else 0 \n                 for p in positions_list]\n    pred_rates = predicted_rate[:23]\n    \n    ax.bar(x - width/2, obs_rates, width, label='Observed', alpha=0.7)\n    ax.bar(x + width/2, pred_rates, width, label='Predicted', alpha=0.7)\n    \n    ax.set_xlabel('Position')\n    ax.set_ylabel(f'{st.capitalize()} per 80 minutes')\n    ax.set_title(f'Observed vs Predicted {st.capitalize()} Rates by Position')\n    ax.set_xticks(x)\n    ax.set_xticklabels([positions.get(p, str(p)) for p in positions_list], rotation=45, ha='right', fontsize=8)\n    ax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Checkpoint\n",
    "\n",
    "Save the fitted model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save checkpoint\ncheckpoint_path = fitter.save(\"joint_model_v1\")\nprint(f\"Saved to: {checkpoint_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# To load later:\n# fitter = ModelFitter.load(\"joint_model_v1\", model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exploring Individual Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Look at a specific player's history and model estimate\ndef analyze_player(player_name, score_type='tries'):\n    \"\"\"Analyze a specific player's career and model estimate.\"\"\"\n    if player_name not in model._player_ids:\n        print(f\"Player '{player_name}' not found in model\")\n        return\n    \n    player_idx = model._player_ids[player_name]\n    \n    # Get player effect from posterior (joint model)\n    score_idx = config.score_types.index(score_type)\n    beta_raw = trace.posterior['beta_player_raw'].values[:, :, player_idx]\n    sigma_player = trace.posterior['sigma_player'].values\n    lambda_player = trace.posterior['lambda_player'].values[:, :, score_idx]\n    \n    # Compute effective player effect for this score type\n    beta = sigma_player * lambda_player * beta_raw\n    \n    # Get player's match history\n    player_df = df[df['player_name'] == player_name].copy()\n    \n    print(f\"=== {player_name} ({score_type}) ===\")\n    print(f\"Matches: {len(player_df)}\")\n    print(f\"Total {score_type}: {player_df[score_type].sum()}\")\n    print(f\"{score_type.capitalize()} rate: {player_df[score_type].sum() / player_df['minutes_played'].sum() * 80:.2f} per 80 min\")\n    print(f\"Teams: {player_df['team'].unique().tolist()}\")\n    print(f\"Seasons: {sorted(player_df['season'].unique().tolist())}\")\n    print()\n    print(f\"Model estimate ({score_type}):\")\n    print(f\"  Effect mean: {beta.mean():.3f}\")\n    print(f\"  Effect std: {beta.std():.3f}\")\n    print(f\"  95% CI: [{np.percentile(beta, 2.5):.3f}, {np.percentile(beta, 97.5):.3f}]\")\n    \n    # Plot posterior for all score types\n    fig, axes = plt.subplots(1, len(config.score_types), figsize=(16, 4))\n    for s, (st, ax) in enumerate(zip(config.score_types, axes)):\n        lambda_p_s = trace.posterior['lambda_player'].values[:, :, s]\n        beta_s = sigma_player * lambda_p_s * beta_raw\n        \n        ax.hist(beta_s.flatten(), bins=50, density=True, alpha=0.7)\n        ax.axvline(x=0, color='gray', linestyle='--', label='League average')\n        ax.axvline(x=beta_s.mean(), color='red', linestyle='-', label=f'Mean: {beta_s.mean():.3f}')\n        ax.set_xlabel('Player Effect')\n        ax.set_ylabel('Density')\n        ax.set_title(f'{st.capitalize()}')\n        ax.legend(fontsize=8)\n    \n    plt.suptitle(f'Posterior Distributions: {player_name}')\n    plt.tight_layout()\n    plt.show()\n\n# Example: analyze top try-scorer\ntop_player = player_rankings.iloc[0]['player']\nanalyze_player(top_player, 'tries')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search for a specific player\ndef search_players(query):\n    \"\"\"Search for players by name.\"\"\"\n    matches = [p for p in model._player_ids.keys() if query.lower() in p.lower()]\n    return sorted(matches)[:20]\n\n# Example search\nprint(\"Players matching 'van der':\", search_players(\"van der\"))"
  },
  {
   "cell_type": "code",
   "source": "# Analyze a known kicker to show the joint model properly values them\n# Search for fly-halves who are known kickers\nkickers = search_players(\"Farrell\") + search_players(\"Sexton\") + search_players(\"Russell\")\nprint(\"Known kickers found:\", kickers[:5])\n\nif kickers:\n    analyze_player(kickers[0], 'penalties')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}